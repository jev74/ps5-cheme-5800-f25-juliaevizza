{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fd3d6b-cedb-4070-84db-9b1d09a0f271",
   "metadata": {},
   "source": [
    "# PS5: The Apples Versus Oranges Problem as a Markov Decision Process\n",
    "Problem set 5 (PS5) tests the hypothesis that a classical optimization problem, such as selecting the optimal consumption bundle of apples and oranges, can be structured as a Markov Decision Process (MDP), where an optimal policy can be computed using value iteration. Toward this objective, there are two problems that we need to solve:\n",
    "\n",
    "* __Problem 1__: In this problem, we solve the apples and oranges problem subject to a budget constraint as a nonlinear programming problem [using the `MadNLP.jl` package](https://github.com/MadNLP/MadNLP.jl). The optimal solution to this problem will be used as the terminal state for the MDP calculations.\n",
    "* __Problem 2__: In this problem, we construct an MDP problem encoding the apples and oranges decision and solve for the optimal policy function $\\pi$ using value iteration. In this problem, we enforce the budget constraint as a soft-wall constraint and use reward shaping to help the search.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5d878-10d4-4af8-85fa-21c55cde92fa",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25550134-a286-4aa1-8aee-19a7bd80d1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ /Users/jvwork/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W2sZmlsZQ==.jl:1:39\ninclude(\"Include-solution.jl\"); change.!\n#                                     â””â”˜ â”€â”€ extra tokens after end of expression",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ /Users/jvwork/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W2sZmlsZQ==.jl:1:39\n",
      "include(\"Include-solution.jl\"); change.!\n",
      "#                                     â””â”˜ â”€â”€ extra tokens after end of expression\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W2sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "include(\"Include-solution.jl\"); change.!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591de334",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777da94-8a5f-45e6-bf2e-ec8abee6fee1",
   "metadata": {},
   "source": [
    "### Helper Function: Cobb-Douglas Utility\n",
    "\n",
    "We need a utility function to evaluate the desirability of different combinations of apples and oranges. The utility function `U(...)` computes the utility of combinations of apples and oranges using a Cobb-Douglas utility model. A tuple holding the number of `(apples, oranges)`, i.e., the combinations of objects we are searching over, and the $\\alpha$-vector (preferences) are passed as arguments; the `utility` value is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca18d71-b870-4cd1-a70b-821265f5de38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function U(x::Tuple{Int,Int}, Î±::Array{Float64,1})::Float64\n",
    "    \n",
    "    # get the apples, and oranges \n",
    "    apples = x[1];\n",
    "    oranges = x[2];\n",
    "    \n",
    "    # compute the objective -\n",
    "    utility = (apples^Î±[1])*(oranges^Î±[2]);\n",
    "    \n",
    "    # return -\n",
    "    return utility;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e49acf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9647f-0576-4a9e-81bf-b01c0ec4ade0",
   "metadata": {},
   "source": [
    "## Problem 1: Compute the optimal number of Apples and Oranges to purchase\n",
    "In this problem, we compute the optimal number of apples and oranges to purchase given a budget constraint using nonlinear programming, i.e., we maximize the utility function subject to the budget constraint.\n",
    "\n",
    "### Problem Statement\n",
    "Use a Cobb-Douglas utility function combined with a budget constraint to compute the optimal combination of apples and oranges that gives the maximum utility for the available budget. The Cobb-Douglas utility function for a collection of objects $x_{1},\\cdots,x_{n}$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{maximize } U(x_{1},\\cdots,x_{n}) &= \\prod_{i=1}^{n}x_{i}^{\\alpha_{i}}\\\\\n",
    "\\text{subject to } \\sum_{i=1}^{n}p_{i}x_{i} &\\leq B\\\\\n",
    "x_{i} &\\geq 0, \\quad i=1,\\cdots,n\n",
    "\\end{align*}\n",
    "$$\n",
    "where the $\\alpha_{i}\\geq{0}$ parameters are preference coefficients associated with each object, $p_{i}$ is the unit price of object $i$, $x_{i}$ are the quantities of each object, and $B$ is the total budget available for purchasing the objects.\n",
    "\n",
    "> __Setup__:\n",
    ">\n",
    "> For this problem, let's assume we have the following parameters:\n",
    "> * The preference coefficient vector $\\alpha = (0.55,0.45)$ and the total budget $B$ = `50 USD`\n",
    "> * The unit price of an `apple` is `0.98 USD` and the unit price of an `orange` is `1.49 USD`\n",
    "> * Let `apples` be index `1` and `oranges` be index `2`\n",
    "> * Assume the bounds run from `0` to $B/p_i$ for each good $i$ and the initial guess is `0.1*ones(2)`.\n",
    "\n",
    "We'll use [the `build(...)` method](src/Factory.jl) to construct an instance of [the `MySimpleCobbDouglasChoiceProblem` type](src/Types.jl) holding the parameters in the `base` variable. We'll then pass `base` to [the `mysolve(...)` function](src/Compute.jl) and set the return to the variable `base_solution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28507283-c742-476c-a301-404d18c21598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `build` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `build` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name also exists in Pkg.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X11sZmlsZQ==.jl:9"
     ]
    }
   ],
   "source": [
    "base_solution, Î±, c, B = let \n",
    "\n",
    "    # initialize -\n",
    "    Î± = [0.55, 0.45]; # coefficients\n",
    "    c = [0.98, 1.49]; # price of x1 and x2\n",
    "    total_budget = 50.0;\n",
    "\n",
    "    # build my problem object -\n",
    "    base = build(MySimpleCobbDouglasChoiceProblem, (\n",
    "    \n",
    "        initial = 0.1*ones(2), # initial guess\n",
    "        Î± = Î±, # coefficients\n",
    "        c = c, # price of x1 and x2\n",
    "        I = total_budget, # income\n",
    "    \n",
    "        # how much of xâ‚ and xâ‚‚ can be we buy?\n",
    "        bounds = [\n",
    "            0.0 total_budget/c[1]; # L U\n",
    "            0.0 total_budget/c[2]; # L U\n",
    "        ]\n",
    "    ));\n",
    "\n",
    "    # call the solve function. This will return a dictionary with data about the solution\n",
    "    base_solution = mysolve(base);\n",
    "\n",
    "    # return -\n",
    "    base_solution, Î±, c, total_budget;\n",
    "    \n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaeb24b-0d4f-491e-83b3-ee11e9c00190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `base_solution` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `base_solution` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] eval(m::Module, e::Any)\n",
      "   @ Core ./boot.jl:489\n",
      " [2] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "   @ Base ./loading.jl:2842\n",
      " [3] (::VSCodeServer.var\"#notebook_runcell_request##0#notebook_runcell_request##1\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:24\n",
      " [4] withpath(f::VSCodeServer.var\"#notebook_runcell_request##0#notebook_runcell_request##1\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/repl.jl:278\n",
      " [5] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint, VSCodeServer.JSON.Serializations.StandardSerialization}, params::VSCodeServer.NotebookRunCellArguments, token::VSCodeServer.CancellationTokens.CancellationToken)\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:13\n",
      " [6] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint, VSCodeServer.JSON.Serializations.StandardSerialization}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::VSCodeServer.JSONRPC.Request)\n",
      "   @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/JSONRPC/src/typed.jl:0\n",
      " [7] serve_notebook(pipename::String, debugger_pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; error_handler::var\"#20#21\"{String})\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:147\n",
      " [8] top-level scope\n",
      "   @ ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/notebook/notebook.jl:28"
     ]
    }
   ],
   "source": [
    "base_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6985ab-a6ef-4b4f-a320-371c140bde07",
   "metadata": {},
   "source": [
    "What is the optimal combination of apples and oranges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f99fa0-70e2-4969-80f4-d5eeb4fd9ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `base_solution` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `base_solution` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "optimal_apples = base_solution[\"argmax\"][1] |> x-> round(x,digits=0) |> Int\n",
    "optimal_oranges = base_solution[\"argmax\"][2] |> x-> round(x,digits=0) |> Int\n",
    "println(\"Optimal: (apples, oranges) = ($(optimal_apples),$(optimal_oranges))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e208e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939af937-2f6d-4a17-add7-e50909c87961",
   "metadata": {},
   "source": [
    "## Problem 2: Solve the Apples and Oranges problem as an MDP\n",
    "Now, we solve the apples versus oranges problem as a Markov Decision Process (MDP). \n",
    "\n",
    "> __What are we going to do?__ \n",
    ">\n",
    "> Here are the subtasks associated with solving the apples versus oranges problem as an MDP:\n",
    "> * __Task 1__: Set up a $30\\times{30}$ grid, encoded as an instance of the `MyRectangularGridWorldModel` type.\n",
    ">   * `TODO`: Add a terminal state at the optimal combination of apples and oranges computed from Problem 1. Set the reward for this state as the optimal _integer_ fitness value calculated using the `U(...)` function defined above.\n",
    ">   * `TODO`: Add the optimal combination of apples and oranges computed from Problem 1 to the `absorbing_state_set`.\n",
    "> * __Task 2__: Use your `MyRectangularGridWorldModel` instance to generate the components of the MDP, namely, the reward function (or array) $R(s, a)$ and the model of the physics of the world in the transition function (or array) $T(s, s^{\\prime}, a)$.\n",
    ">    * `TODO`: Modify the $R[s,a]$ array code from lecture `L11d` so that it uses the `U(...)` function for its values. This is a type of reward shaping, as we use the utility function model to give the agent some hints along the way.\n",
    ">    * `TODO`: Modify the $R[s,a]$ array to describe a _soft wall_, i.e., a region where the budget constraint is violated. Unlike a hard wall that makes states unreachable, a soft wall assigns a penalty but still allows the agent to explore these states. Set the wall penalty as `-1000` and allow up to a `1 USD` violation of the budget constraint before applying the penalty.\n",
    "> * __Task 3__: Use value iteration to estimate the optimal value function $U^{\\star}(s)$. \n",
    ">    * `TODO`: For your choice of the $(\\gamma,k_{\\max},\\epsilon)$ hyperparameters, extract the action-value function $Q(s, a)$ from the optimal value function $U^{\\star}(s)$ and compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$.\n",
    "\n",
    "Let's implement these tasks step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08abf7f-ed7d-4a72-be89-ddb403e1bc06",
   "metadata": {},
   "source": [
    "### Task 1: Build the Apples and Oranges world model\n",
    "In this task, we build the apples and oranges world model as a rectangular grid world. We encode the rectangular grid world as an instance of the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. \n",
    "\n",
    "First, let's set up the data for the apples and oranges world, i.e., set up the states, actions, and rewards, and then construct the world model. \n",
    "* `TODO`: Set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` available to the agent, and the discount factor $\\gamma$.\n",
    "* `TODO`: Compute the number of states and set up the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$.\n",
    "\n",
    "Update any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "656b80a2-e389-49ed-ae8b-8e69b164d4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 30 # set the number of rows in the grid\n",
    "number_of_cols = 30 # set the number of columns in the grid\n",
    "nactions = 4; # set the number of actions available to the agent\n",
    "nstates = (number_of_rows*number_of_cols); # this is the dimension of the state space\n",
    "ð’® = range(1,stop=nstates,step=1) |> collect; # we number the states from 1 to nstates\n",
    "ð’œ = range(1,stop=nactions,step=1) |> collect; # we number the actions from 1 to nactions\n",
    "Î³ = 0.95; # set the discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ba399-75e0-4be9-88a9-e973151aa607",
   "metadata": {},
   "source": [
    "Next, set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. \n",
    "  * `TODO`: Add a terminal state at the optimal combination of apples and oranges computed from Problem 1. Set the reward for this state to be the optimal _integer_ fitness value (reported above), using the `U(...)` function.\n",
    "  * `TODO`: Add the optimal combination of apples and oranges computed from Problem 1 to the `absorbing_state_set::Set{Tuple{Int,Int}}.` If we arrive at an absorbing state, we stay there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a53a314-6fa1-4ac3-addd-36b1aad0076c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `optimal_apples` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `optimal_apples` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X23sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_objective_value = U((optimal_apples, optimal_oranges), Î±)\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(optimal_apples, optimal_oranges)] = my_objective_value;\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (optimal_apples, optimal_oranges));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc44527-806d-442a-b915-42a81d06890d",
   "metadata": {},
   "source": [
    "Finally, build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. \n",
    "\n",
    "Pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into [the `build(...)` method](src/Factory.jl). Save the world model instance to the `world::MyRectangularGridWorldModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a91e120-7ca3-48cf-8ee3-ff62ae899c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `build` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `build` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name also exists in Pkg.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X25sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "world = build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b2d22f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `world` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `world` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X26sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "println(\"Grid world created with $(world.number_of_rows)Ã—$(world.number_of_cols) = $nstates states\")\n",
    "println(\"Terminal state set at: (apples, oranges) = ($(optimal_apples), $(optimal_oranges))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bbac2",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf02232-2971-48a3-a8a1-90e5619d1299",
   "metadata": {},
   "source": [
    "### Task 2: Generate the components of the MDP problem\n",
    "In this task, we generate the components of the MDP problem from our `MyRectangularGridWorldModel` instance. The MDP problem requires the reward function (or array) $R(s, a)$ and the transition function (or array) $T(s, s^{\\prime}, a)$. Let's construct these from our grid world model instance, starting with the $R(s, a)$ reward function.\n",
    "\n",
    "#### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the __immediate__ reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the reward `R::Array{Float64,2}` array with zeros, populate the non-zero values of $R(s, a)$ using a nested [for loop](https://docs.julialang.org/en/v1/base/base/#for). During each outer loop iteration, we select a state $s\\in\\mathcal{S}$, and the inner loop iterates over actions $a\\in\\mathcal{A}$.\n",
    "\n",
    "For each state `s` and action `a` with corresponding move $\\Delta$:\n",
    "* Compute the new position resulting from implementing action `a` and store this in the `new_position` variable. If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary, we use that reward value. If we are still in the world but not in a special location, we set the reward to `-1`. If `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`.\n",
    "\n",
    "#### Modifications\n",
    "The implementation below will be similar to lab `L11d`, but with a few modifications:\n",
    "* `TODO`: Modify the $R[s,a]$ array from `L11d` so that it uses the `U(...)` function for the default values. This is a type of reward shaping.\n",
    "* `TODO`: Modify the $R[s, a]$ array to describe a `soft wall`, i.e., a region where the budget constraint is violated. Set the `wall` penalty as `-1000`. Allow up to a `1 USD` violation of the budget constraint. The off-the-grid penalty is set to `-50000.0`.\n",
    "\n",
    "Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4885706",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `B` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `B` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X32sZmlsZQ==.jl:4"
     ]
    }
   ],
   "source": [
    "R, soft_wall_set = let\n",
    "\n",
    "    # initialize -\n",
    "    total_budget = B; # alias the total budget\n",
    "    R = zeros(nstates, nactions);\n",
    "    fill!(R, 0.0)\n",
    "    off_grid_penalty = -50000.0; # this is the penalty for going off the grid\n",
    "    softwall_penalty = -1000.0; # this is the penalty for hitting a soft wall\n",
    "\n",
    "    for s âˆˆ ð’®\n",
    "        for a âˆˆ ð’œ\n",
    "            \n",
    "            Î” = world.moves[a];\n",
    "            current_position = world.coordinates[s]\n",
    "            new_position =  current_position .+ Î”\n",
    "            \n",
    "            if (haskey(world.states, new_position) == true)\n",
    "                if (haskey(rewards, new_position) == true)\n",
    "                    R[s,a] = rewards[new_position];\n",
    "                else\n",
    "                    R[s,a] = U(new_position, Î±);\n",
    "                end\n",
    "            else\n",
    "                R[s,a] = off_grid_penalty; # we are off the grid, big negative penalty\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # setup soft walls -\n",
    "    soft_wall_set = Set{Tuple{Int,Int}}();\n",
    "    for s âˆˆ ð’®\n",
    "        \n",
    "        # get the position -\n",
    "        current_position = world.coordinates[s]\n",
    "        \n",
    "        # does this position violate the budget?\n",
    "        budget_violation = max(0.0, c[1]*current_position[1]+c[2]*current_position[2] - total_budget)\n",
    "        if (budget_violation â‰¥ 1.0)\n",
    "            push!(soft_wall_set, current_position)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for s âˆˆ ð’®\n",
    "        current_position = world.coordinates[s]\n",
    "        for a âˆˆ ð’œ\n",
    "            Î” = world.moves[a];\n",
    "            new_position =  current_position .+ Î”\n",
    "            \n",
    "            if (in(new_position, soft_wall_set) == true)\n",
    "                R[s,a] = softwall_penalty; # penalty for hitting a soft wall  \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    R, soft_wall_set; # return \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bba6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `R` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `R` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X33sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "println(\"Reward matrix R dimensions: $(size(R))\")\n",
    "println(\"Number of soft wall states: $(length(soft_wall_set))\")\n",
    "println(\"Soft wall states represent budget violations â‰¥ 1 USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164cc27-d821-469c-bb3d-0ada67b2ff0c",
   "metadata": {},
   "source": [
    "#### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimensional array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. I've already done this, so back away slowly from your keyboard and move on to the next task!\n",
    "\n",
    "But if you're curious, here's how it works:\n",
    "* In the `outer` loop, we iterate over actions. For every $a\\in\\mathcal{A}$, we get the move associated with that action and store it in the `Î”::Tuple`.\n",
    "* In the `inner` loop, we iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world and `current_position` is _not_ an `absorbing state`, we set $s^{\\prime}\\leftarrow$`world.states[new_position]` and `T[s, sâ€²,  a] = 1.0`.\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923e659b-22c8-4a3e-ab86-077b91e489b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `world` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `world` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X35sZmlsZQ==.jl:11"
     ]
    }
   ],
   "source": [
    "T = let\n",
    "\n",
    "    # --- DO NOT CHANGE ME .... PUT YOUR HANDS DOWN, AND SLOWLY STEP AWAY FROM YOUR KEYBOARD --------------- #\n",
    "    \n",
    "    # initialize -\n",
    "    T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "    fill!(T, 0.0);\n",
    "\n",
    "    # main loop -\n",
    "    for a âˆˆ ð’œ\n",
    "        Î” = world.moves[a]; # for this action, get the coordinate move\n",
    "        for s âˆˆ ð’®\n",
    "            current_position = world.coordinates[s]\n",
    "            new_position =  current_position .+ Î”\n",
    "            if (haskey(world.states, new_position) == true && \n",
    "                    in(current_position, absorbing_state_set) == false)\n",
    "                sâ€² = world.states[new_position];\n",
    "                T[s, sâ€²,  a] = 1.0\n",
    "            else\n",
    "                T[s, s,  a] = 1.0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # ----------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "    T # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5feb6e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `T` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `T` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X36sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "println(\"Transition matrix T dimensions: $(size(T))\")\n",
    "println(\"T encodes deterministic transitions between states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f5445",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4f378-f223-4c44-9f17-abd0e41b6404",
   "metadata": {},
   "source": [
    "## Task 3: Use value iteration to estimate the optimal value function $U^{\\star}(s)$\n",
    "In Task 3, we construct a Markov Decision Process (MDP) instance and solve the problem using value iteration. The solution of the value iteration procedure is then used to estimate the optimal policy $\\pi^{\\star}(s)$. Toward this task:\n",
    "\n",
    "Construct an instance of the `MyMDPProblemModel`, which encodes the data required to solve the MDP problem. Pass the states `ð’®`, the actions `ð’œ`, the transition matrix `T`, the reward matrix `R`, and the discount factor `Î³` into [the `build(...)` method](src/Factory.jl). We store the MDP model in the `m::MyMDPProblemModel` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bee066c6-f228-4d4d-a9ad-079cb6995b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `build` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Pkg.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `build` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name also exists in Pkg.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X42sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "m = build(MyMDPProblemModel, (ð’® = ð’®, ð’œ = ð’œ, T = T, R = R, Î³ = Î³)); # build an mdp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b741bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP model constructed with:\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `m` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `m` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X43sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "println(\"MDP model constructed with:\")\n",
    "println(\"  - State space size: $(length(m.ð’®))\")\n",
    "println(\"  - Action space size: $(length(m.ð’œ))\")\n",
    "println(\"  - Discount factor Î³: $(m.Î³)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb505c65-f9a2-4e9a-874b-8a7b06f5aec5",
   "metadata": {},
   "source": [
    "Next, call [the `mysolve(...)` method](src/Compute.jl) by passing a `value_iteration_model` instance and our MDP model `m::MyMDPProblemModel` as arguments. [The `mysolve(...)` method](src/Compute.jl) iteratively computes the optimal value function $U^{\\star}(s)$ and returns it in an instance of [the `MyValueIterationSolution` type](src/Types.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e41d060-5c23-4e9f-9c11-af63fa3a33f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `MyValueIterationModel` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `MyValueIterationModel` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X45sZmlsZQ==.jl:7"
     ]
    }
   ],
   "source": [
    "solution = let\n",
    "\n",
    "    # initialize - \n",
    "    solution = nothing;\n",
    "    k_max = 2500; # maximum number of iterations\n",
    "    Ïµ = 1e-8; # convergence tolerance\n",
    "    value_iteration_model = MyValueIterationModel(k_max); # takes k_max as argument\n",
    "\n",
    "    # TODO: Call the mysolve(...) function to solve the MDP\n",
    "    solution = mysolve(value_iteration_model, m, Ïµ=Ïµ);\n",
    "\n",
    "    # solution -\n",
    "    solution\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5d41c-b7c2-4ed4-bf8d-ed40a9eebdd7",
   "metadata": {},
   "source": [
    "Now, we extract the action-value function $Q(s, a)$ from the optimal value function $U^{\\star}(s)$. First, we estimate $Q(s,a)$ as:\n",
    "\n",
    "> __Policy__: To extract the policy, we need to compute the __state-action value function__ $Q(s,a)$ for each state and action. To estimate the policy, for each state $s \\in \\mathcal{S}$, compute:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\pi^{*}(s) &\\gets \\arg\\max_{a\\in\\mathcal{A}_s}\\left(\\underbrace{R(s,a) + \\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}T\\left(s^{\\prime}\\,|\\,s,a\\right)\\cdot{U^{*}}(s^{\\prime})}_{Q(s,a)\\text{ = brain of agent}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "> We can do this using [the `QM(...)` function](src/Compute.jl), which takes `m::MyMDPProblemModel` and the `solution::MyValueIterationSolution`. \n",
    "\n",
    "Save the optimal $Q(s,a)$ in the `my_Q::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c091d3c-e3b9-4bd3-8195-8220d58fecce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `QM` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `QM` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X50sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_Q = QM(m, solution.U) # this computes the state-action value function Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d8d92-5880-49d6-910d-68d99da2a1fb",
   "metadata": {},
   "source": [
    "Finally, compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$ using [the `mypolicy(...)` function](src/Compute.jl). Save this in the `my_Ï€::Array{Int64,1}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6468d7ad-fac1-422b-b7f6-4cf7619a0544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `mypolicy` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `mypolicy` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "my_Ï€ = mypolicy(my_Q) # this tells us what we should do at each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b871ac79",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `my_Ï€` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_Ï€` not defined in `Main`\n",
      "Suggestion: add an appropriate import or assignment. This global was declared but not assigned.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X53sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "println(\"Optimal policy Ï€ computed for all $(length(my_Ï€)) states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaf91e-e201-43c2-8b84-d37a53df9c57",
   "metadata": {},
   "source": [
    "### Visualize the optimal policy\n",
    "The code block below shows how we visualize the decision-maker's path through the apples versus oranges space to arrive at the optimal solution. This code to visualize the optimal policy $\\pi^{\\star}(s)$ was modified from `L11d`.\n",
    "\n",
    "Specify an initial starting tuple `(apples, oranges)` in the `initial_site` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "338c7620-056d-4be0-bd96-e4434c004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_site = (1, 30); # horizontal, vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86f775",
   "metadata": {},
   "source": [
    "This code block generates the plot showing the optimal navigation path from the starting position to the goal while avoiding hazards. `Unhide` the cell to see the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4edb0539-29cb-4b00-a58e-2b4a370765bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `plot` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X60sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # draw the path -\n",
    "    p = plot();\n",
    "    hit_absorbing_state = false\n",
    "    s = world.states[initial_site];\n",
    "    visited_sites = Set{Tuple{Int,Int}}();\n",
    "    push!(visited_sites, initial_site);\n",
    "    \n",
    "    while (hit_absorbing_state == false)\n",
    "        current_position = world.coordinates[s]\n",
    "        a = my_Ï€[s];\n",
    "        Î” = world.moves[a];\n",
    "        new_position =  current_position .+ Î”\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "        plot!([current_position[1], new_position[1]],[current_position[2], new_position[2]], label=\"\", arrow=true, lw=1, c=:red)\n",
    "        \n",
    "        if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "            hit_absorbing_state = true;\n",
    "        else\n",
    "            s = world.states[new_position];\n",
    "            push!(visited_sites, new_position);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # draw the grid -\n",
    "    for s âˆˆ ð’®\n",
    "        current_position = world.coordinates[s]\n",
    "        a = my_Ï€[s];\n",
    "        Î” = world.moves[a];\n",
    "        new_position =  current_position .+ Î”\n",
    "        \n",
    "        if (haskey(rewards, current_position) == true && rewards[current_position] == my_objective_value)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"Optimal: $(current_position)\", c=:green, ms=4, legend=:bottomleft)\n",
    "        elseif (in(current_position, soft_wall_set) == true)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "        else\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:gray50, c=:white)\n",
    "        end\n",
    "    end\n",
    "    xlabel!(\"Number of Apples\",fontsize=18)\n",
    "    ylabel!(\"Number of Oranges\",fontsize=18)\n",
    "    current()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e5f44",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "The plot above shows the optimal navigation path (red arrows with blue dots) from the starting position to the optimal consumption bundle (green dot). Notice the following key features:\n",
    "* The __green dot__ marks the optimal combination of apples and oranges computed in Problem 1.\n",
    "* The __gray region__ represents the soft-wall states where the budget constraint is violated by â‰¥ 1 USD.\n",
    "* The __optimal path__ (red arrows) navigates through feasible states, avoiding the budget-violating region, and terminates at the goal state.\n",
    "* The policy guides the agent from any starting position toward the optimal consumption bundle while respecting the budget constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10ad87",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1f0a",
   "metadata": {},
   "source": [
    "### Exploration: Parameter Sensitivity Analysis\n",
    "Now that you have a working MDP solution, explore how different parameters affect the optimal policy and convergence behavior. Answer the following questions by modifying the appropriate parameters in the notebook above and re-running the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905db0d",
   "metadata": {},
   "source": [
    "#### Question 1: Effect of the Discount Factor\n",
    "What happens to the optimal policy when you decrease the discount factor $\\gamma$ from `0.95` to `0.50`? \n",
    "\n",
    "__Task__: Change $\\gamma$ to `0.50`, re-run the MDP construction and value iteration cells, and observe the resulting policy. Does the agent still navigate efficiently to the optimal consumption bundle? Why or why not?\n",
    "\n",
    "__Your Answer__: _(Write your observations and explanation here)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84b1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to true once you have answered Question 1\n",
    "did_I_answer_question_1 = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838155e8",
   "metadata": {},
   "source": [
    "#### Question 2: Soft-Wall Tolerance Impact\n",
    "How does changing the soft-wall tolerance affect the size of the feasible region and the optimal policy?\n",
    "\n",
    "__Task__: Modify the budget violation threshold from `1.0 USD` to `5.0 USD` in the soft-wall setup (where we check `if (budget_violation â‰¥ 1.0)`). Re-run the reward matrix construction and subsequent cells. How does the size of `soft_wall_set` change? Does the optimal path change significantly?\n",
    "\n",
    "__Your Answer__: _(Write your observations and explanation here)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9976913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to true once you have answered Question 2\n",
    "did_I_answer_question_2 = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13a18",
   "metadata": {},
   "source": [
    "#### Question 3: Grid Resolution and Computational Cost\n",
    "What is the trade-off between grid resolution and computational performance?\n",
    "\n",
    "__Task__: Change the grid size from $30\\times 30$ to $50\\times 50$ by modifying `number_of_rows` and `number_of_cols`. Re-run all relevant cells from Task 1 onward. Compare the computation time for value iteration and observe whether the optimal consumption bundle changes (it should remain approximately the same since Problem 1 is independent of the grid). What is the computational cost of increasing the state space?\n",
    "\n",
    "__Hint__: You can measure execution time in Julia using the `@elapsed` macro, e.g., `time_taken = @elapsed mysolve(value_iteration_model, m)`.\n",
    "\n",
    "__Your Answer__: _(Write your observations and explanation here)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33a67532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to true once you have answered Question 3\n",
    "did_I_answer_question_3 = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689d604",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135246ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We demonstrated that the apples and oranges resource allocation problem can be formulated and solved as a Markov Decision Process using value iteration.\n",
    "\n",
    "> Key takeaways:\n",
    "> * The optimal consumption bundle from nonlinear programming (Problem 1) served as the terminal state for the MDP, providing a clear goal for the value iteration algorithm to converge toward.\n",
    "> * Reward shaping with the Cobb-Douglas utility function $U(x)$ guided the MDP search by providing intermediate feedback, while soft-wall penalties enforced the budget constraint without hard boundaries.\n",
    "> * Value iteration successfully computed an optimal policy $\\pi^{\\star}(s)$ that navigates from any starting state to the optimal consumption bundle while respecting the budget constraint.\n",
    "\n",
    "This approach shows how classical optimization problems can be reframed as sequential decision-making problems, opening pathways to solve more complex resource allocation scenarios with uncertainty and dynamic constraints.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944d4db",
   "metadata": {},
   "source": [
    "## Tests\n",
    "The code block below shows how we implemented the tests and what we are testing. In these tests, we check values in your notebook and give feedback on which items are correct, missing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42673044",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@testset` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at /Users/jvwork/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y106sZmlsZQ==.jl:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@testset` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "in expression starting at /Users/jvwork/Documents/GitHub/ps5-cheme-5800-f25-juliaevizza/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y106sZmlsZQ==.jl:1\n",
      "\n",
      "Stacktrace:\n",
      " [1] eval(m::Module, e::Any)\n",
      "   @ Core ./boot.jl:489\n",
      " [2] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "   @ Base ./loading.jl:2842\n",
      " [3] (::VSCodeServer.var\"#notebook_runcell_request##0#notebook_runcell_request##1\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:24\n",
      " [4] withpath(f::VSCodeServer.var\"#notebook_runcell_request##0#notebook_runcell_request##1\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/repl.jl:278\n",
      " [5] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint, VSCodeServer.JSON.Serializations.StandardSerialization}, params::VSCodeServer.NotebookRunCellArguments, token::VSCodeServer.CancellationTokens.CancellationToken)\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:13\n",
      " [6] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint, VSCodeServer.JSON.Serializations.StandardSerialization}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::VSCodeServer.JSONRPC.Request)\n",
      "   @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/JSONRPC/src/typed.jl:0\n",
      " [7] serve_notebook(pipename::String, debugger_pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; error_handler::var\"#20#21\"{String})\n",
      "   @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:147\n",
      " [8] top-level scope\n",
      "   @ ~/.vscode/extensions/julialang.language-julia-1.158.2/scripts/notebook/notebook.jl:28"
     ]
    }
   ],
   "source": [
    "@testset verbose = true \"CHEME 5800 PS5 Test Suite\" begin\n",
    "\n",
    "    @testset \"Problem 1: Optimal Apples and Oranges (NLP)\" begin\n",
    "        \n",
    "        # Test that base_solution exists and has the correct structure\n",
    "        @test @isdefined(base_solution)\n",
    "        @test isa(base_solution, Dict)\n",
    "        @test haskey(base_solution, \"argmax\")\n",
    "        @test haskey(base_solution, \"objective_value\")\n",
    "        \n",
    "        # Test that optimal values are defined and are integers\n",
    "        @test @isdefined(optimal_apples)\n",
    "        @test @isdefined(optimal_oranges)\n",
    "        @test isa(optimal_apples, Int)\n",
    "        @test isa(optimal_oranges, Int)\n",
    "        \n",
    "        # Test that optimal values are non-negative\n",
    "        @test optimal_apples â‰¥ 0\n",
    "        @test optimal_oranges â‰¥ 0\n",
    "        \n",
    "        # Test that the solution respects reasonable bounds (not exact due to parameter flexibility)\n",
    "        @test optimal_apples < 100  # reasonable upper bound\n",
    "        @test optimal_oranges < 100 # reasonable upper bound\n",
    "        \n",
    "        # Test that Î±, c, and B are defined with correct dimensions\n",
    "        @test @isdefined(Î±)\n",
    "        @test @isdefined(c)\n",
    "        @test @isdefined(B)\n",
    "        @test length(Î±) == 2\n",
    "        @test length(c) == 2\n",
    "        @test B > 0\n",
    "    end\n",
    "\n",
    "    @testset \"Task 1: Grid World Model Setup\" begin\n",
    "        \n",
    "        # Test that grid dimensions are defined\n",
    "        @test @isdefined(number_of_rows)\n",
    "        @test @isdefined(number_of_cols)\n",
    "        @test number_of_rows > 0\n",
    "        @test number_of_cols > 0\n",
    "        \n",
    "        # Test that state and action spaces are defined correctly\n",
    "        @test @isdefined(nstates)\n",
    "        @test @isdefined(nactions)\n",
    "        @test @isdefined(ð’®)\n",
    "        @test @isdefined(ð’œ)\n",
    "        @test nstates == number_of_rows * number_of_cols\n",
    "        @test length(ð’®) == nstates\n",
    "        @test length(ð’œ) == nactions\n",
    "        @test nactions == 4  # 4 cardinal directions\n",
    "        \n",
    "        # Test that discount factor is defined and in valid range\n",
    "        @test @isdefined(Î³)\n",
    "        @test 0 < Î³ â‰¤ 1\n",
    "        \n",
    "        # Test that rewards dictionary is defined and contains the optimal state\n",
    "        @test @isdefined(rewards)\n",
    "        @test isa(rewards, Dict)\n",
    "        @test haskey(rewards, (optimal_apples, optimal_oranges))\n",
    "        \n",
    "        # Test that absorbing state set is defined and contains the optimal state\n",
    "        @test @isdefined(absorbing_state_set)\n",
    "        @test isa(absorbing_state_set, Set)\n",
    "        @test in((optimal_apples, optimal_oranges), absorbing_state_set)\n",
    "        \n",
    "        # Test that world model is constructed correctly\n",
    "        @test @isdefined(world)\n",
    "        @test isa(world, MyRectangularGridWorldModel)\n",
    "        @test world.number_of_rows == number_of_rows\n",
    "        @test world.number_of_cols == number_of_cols\n",
    "        @test !isempty(world.coordinates)\n",
    "        @test !isempty(world.states)\n",
    "        @test !isempty(world.moves)\n",
    "    end\n",
    "\n",
    "    @testset \"Task 2: MDP Components (R and T matrices)\" begin\n",
    "        \n",
    "        # Test that R matrix is defined with correct dimensions\n",
    "        @test @isdefined(R)\n",
    "        @test isa(R, Array{Float64,2})\n",
    "        @test size(R) == (nstates, nactions)\n",
    "        \n",
    "        # Test that soft wall set is defined\n",
    "        @test @isdefined(soft_wall_set)\n",
    "        @test isa(soft_wall_set, Set)\n",
    "        \n",
    "        # Test that T matrix is defined with correct dimensions\n",
    "        @test @isdefined(T)\n",
    "        @test isa(T, Array{Float64,3})\n",
    "        @test size(T) == (nstates, nstates, nactions)\n",
    "        \n",
    "        # Test that T is a valid probability distribution (rows sum to 1 for each action)\n",
    "        for a in ð’œ\n",
    "            for s in ð’®\n",
    "                @test sum(T[s, :, a]) â‰ˆ 1.0 atol=1e-10\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Test that all T values are non-negative\n",
    "        @test all(T .â‰¥ 0)\n",
    "        \n",
    "        # Test that MDP model is constructed\n",
    "        @test @isdefined(m)\n",
    "        @test isa(m, MyMDPProblemModel)\n",
    "        @test m.ð’® == ð’®\n",
    "        @test m.ð’œ == ð’œ\n",
    "        @test m.Î³ == Î³\n",
    "    end\n",
    "\n",
    "    @testset \"Task 3: Value Iteration and Policy Extraction\" begin\n",
    "        \n",
    "        # Test that solution is defined and has correct structure\n",
    "        @test @isdefined(solution)\n",
    "        @test hasfield(typeof(solution), :U)\n",
    "        \n",
    "        # Test that value function U has correct dimensions\n",
    "        @test length(solution.U) == nstates\n",
    "        \n",
    "        # Test that Q matrix is defined with correct dimensions\n",
    "        @test @isdefined(my_Q)\n",
    "        @test isa(my_Q, Array{Float64,2})\n",
    "        @test size(my_Q) == (nstates, nactions)\n",
    "        \n",
    "        # Test that policy is defined with correct dimensions\n",
    "        @test @isdefined(my_Ï€)\n",
    "        @test isa(my_Ï€, Array{Int64,1})\n",
    "        @test length(my_Ï€) == nstates\n",
    "        \n",
    "        # Test that all policy actions are valid (in action space)\n",
    "        @test all(a -> a âˆˆ ð’œ, my_Ï€)\n",
    "        \n",
    "        # Test that initial site is defined\n",
    "        @test @isdefined(initial_site)\n",
    "        @test isa(initial_site, Tuple{Int,Int})\n",
    "        @test initial_site[1] > 0 && initial_site[1] â‰¤ number_of_cols\n",
    "        @test initial_site[2] > 0 && initial_site[2] â‰¤ number_of_rows\n",
    "    end\n",
    "\n",
    "    @testset \"Helper Functions\" begin\n",
    "        \n",
    "        # Test the U function exists and works correctly\n",
    "        @test @isdefined(U)\n",
    "        \n",
    "        # Test U function with simple inputs\n",
    "        test_result = U((1, 1), [0.5, 0.5])\n",
    "        @test isa(test_result, Float64)\n",
    "        @test test_result â‰¥ 0\n",
    "        \n",
    "        # Test U function with the optimal solution\n",
    "        optimal_utility = U((optimal_apples, optimal_oranges), Î±)\n",
    "        @test isa(optimal_utility, Float64)\n",
    "        @test optimal_utility > 0\n",
    "    end\n",
    "\n",
    "    @testset \"Questions\" begin\n",
    "        @test did_I_answer_question_1 == true\n",
    "        @test did_I_answer_question_2 == true\n",
    "        @test did_I_answer_question_3 == true\n",
    "    end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1084bf91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
